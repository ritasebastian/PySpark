Here's a basic **PySpark** code snippet to test in **Google Colab**, covering essential operations like **RDDs, DataFrames, Transformations, and Actions**—good for **interview preparation**.

### Steps:
1. Install PySpark in Google Colab.
2. Initialize a Spark Session.
3. Work with RDDs and DataFrames.
4. Perform basic transformations and actions.

---

### **Basic PySpark Code for Google Colab**
```python
# Install PySpark (if not already installed)
!pip install pyspark

# Import necessary libraries
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, sum, avg

# Initialize Spark Session
spark = SparkSession.builder.appName("PySparkInterview").getOrCreate()

# Sample Data for DataFrame
data = [("Alice", 25, "Sales"),
        ("Bob", 30, "HR"),
        ("Charlie", 28, "IT"),
        ("David", 35, "Sales"),
        ("Eve", 29, "HR")]

columns = ["Name", "Age", "Department"]

# Create DataFrame
df = spark.createDataFrame(data, columns)

# Show Data
df.show()

# Basic Transformations & Actions
df_filtered = df.filter(col("Age") > 28)
df_filtered.show()

df_grouped = df.groupBy("Department").agg(avg("Age").alias("Avg_Age"))
df_grouped.show()

# Working with RDD
rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])
rdd_squared = rdd.map(lambda x: x ** 2)
print("Squared RDD:", rdd_squared.collect())

# Stop Spark Session
spark.stop()
```

---

### **Key Concepts Covered:**
✅ **Creating SparkSession**  
✅ **Creating DataFrames**  
✅ **Filtering Data** (`filter()`)  
✅ **Aggregations** (`groupBy().agg()`)  
✅ **RDD Operations** (`map()`, `collect()`)  

